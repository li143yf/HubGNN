import argparse
import configparser
from datetime import datetime
import csv
import copy
import random
import numpy as np
import pickle
import torch
import networkx as nx
import os
import dgl
import numpy as np 
from ogb.nodeproppred import PygNodePropPredDataset
from ogb.nodeproppred import DglNodePropPredDataset
from dgl.data import citation_graph
from models.model import HubGNN
from lib.utils_large import train_model
from torch_geometric.datasets import Planetoid, WikiCS
from torch_geometric.transforms import NormalizeFeatures
from torch_geometric.utils import to_networkx, add_self_loops
from torch_geometric.transforms import NormalizeFeatures


#*******************************MAIN BODY*************************************#

MODE = 'train'
DEBUG = 'False'
DATASET = 'cora'
DEVICE = 'cuda:0'


config_file = 'graph.conf'
print('Read Configuration file: %s' % (config_file))
config = configparser.ConfigParser()
config.read(config_file)

args = argparse.ArgumentParser(description='arguments')


args.add_argument('--mode', default=MODE, type=str)
args.add_argument('--device', default=config['log']['device'], type=str, help='indices of GPUs')
args.add_argument('--debug', default=config['log']['debug'], type=eval)
args.add_argument('--cuda', default=True, type=bool)
args.add_argument("--seed", default=config['data']['seed'], help="seed",type=int)
args.add_argument("--dataset", default=config['data']['dataset'], help="dataset", type=str)
args.add_argument("--num_parts", default=config['data']['num_parts'], help="num_clusters",type=int)
args.add_argument("--batch_size", default=config['training']['batch_size'], type=int)
args.add_argument('--model', default=config['model']['model'], type=str)
args.add_argument('--epoch', default=config['model']['epoch'], type=int)
args.add_argument('--lr', default=config['model']['lr'], type=float)
args.add_argument('--num_layers', default=config['model']['num_layers'], type=int)
args.add_argument('--gnn_hidden_dim', default=config['model']['gnn_hidden_dim'], type=int)
args.add_argument('--init_memory', default=config['model']['init_memory'], type=eval)
args.add_argument('--shared_memory_attention', default=config['model']['shared_memory_attention'], type=eval)
args.add_argument('--mem_slots', default=config['model']['mem_slots'], type=int)
args.add_argument('--encoder_attention_heads', default=config['model']['encoder_attention_heads'], type=int)
args.add_argument('--encoder_embed_dim', default=config['model']['encoder_embed_dim'], type=int)
args.add_argument('--encoder_ffn_embed_dim', default=config['model']['encoder_ffn_embed_dim'], type=int)
args.add_argument('--attention_dropout', default=config['model']['attention_dropout'], type=float)
args.add_argument('--topk_ratio', default=config['model']['topk_ratio'], type=float)
args.add_argument('--encoder_normalize_before', default=config['model']['encoder_normalize_before'], type=eval)
args.add_argument('--use_nfm', default=config['model']['use_nfm'], type=eval)
args.add_argument('--null_attention', default=config['model']['null_attention'], type=eval)
args.add_argument('--self_attention', default=config['model']['self_attention'], type=eval)
args.add_argument('--use_topk', default=config['model']['use_topk'], type=eval)
args.add_argument('--topk', default=config['model']['topk'], type=int)
args.add_argument('--num_steps', default=config['model']['topk'], type=int)
args.add_argument('--regressive', default=config['model']['regressive'], type=eval)
args.add_argument('--dropout', default=config['model']['dropout'], type=float)
args = args.parse_args()
current_time = datetime.now().strftime('%mM%dD%H:%M')
current_dir = os.path.dirname(os.path.realpath(__file__))






###########Dataset
dataset1 = dgl.data.AsNodePredDataset(DglNodePropPredDataset("ogbn-arxiv", root="./dataset/ogbn-arxiv"))
graph = dataset1[0]
num_edges = graph.num_edges()  
num_edges = graph.num_edges() 
num_nodes = graph.num_nodes()
print("G_NODE", num_nodes)
print("G_EDGE", num_edges)

node_features = graph.ndata["feat"]
node_num = graph.ndata["feat"].size(0)
num_features = node_features.shape[1]
node_labels = graph.ndata["label"]
num_labels = int(node_labels.max().item() + 1)
modelclass = HubGNN
final_test_ac = train_model(modelclass, 
                                    args.num_layers,
                                    graph.ndata["train_mask"], 
                                    graph.ndata["val_mask"], 
                                    graph.ndata["test_mask"], 
                                    graph,  
                                    node_features,
                                    node_labels,
                                    num_features,
                                    num_labels,
                                    epoch_num=args.epoch, 
                                    groups=None,
                                    seed=args.seed, 
                                    lr=args.lr,
                                    device=args.device,
                                    N_P=args.num_parts,
                                    B_S=args.batch_size,
                                    args=args,                                    
                                    logger=None)

